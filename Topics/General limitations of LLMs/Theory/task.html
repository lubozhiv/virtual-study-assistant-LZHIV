<div class="step-text">
<p>You have already learned that Large Language Models are suited for tasks like producing text or code. Well, despite all their advantages, sometimes LLMs can be misleading!</p><p>In this topic, you'll learn how and when they tend to provide incorrect information and struggle with understanding complex situations. You'll also learn about these limitations affecting their practical usage.</p><h5 id="knowledge-cut-offs">Knowledge cut-offs</h5><p>Large language models like GPT-3 and Claude have a <strong>knowledge cut-off</strong>, meaning they're unaware of events after their last update. This is a challenge in software development, where new syntax, frameworks, and libraries develop rapidly, and outdated code can lead to errors.</p><p>ChatGPT-4, a paid version provided by OpenAI,  has addressed this problem by incorporating the '<em>Browse with Bing</em>' feature. This feature allows ChatGPT-4 to search the web directly and answer questions that require more recent information. With the release of GPT-4o, the web search functionality is now included by default, ensuring users have access to the most current data. Other LLMs like Perplexity AI and Google Gemini also are able to get the latest information via web search.</p><h5 id="hallucinations">Hallucinations</h5><p>Hallucination occurs when a language model gives incorrect results for your prompts. For example, in the prompt snippet below, you may ask about the implementation of the vector cross product for complex numbers from the Eigen library:</p><prompt-snippet><prompt-request data-mobile-hidden="true">Please provide a code snippet implementing the vector cross product for complex numbers in the Eigen template library.</prompt-request><prompt-completion data-mobile-hidden="true" model="gpt-4o">&lt;p&gt;Eigen does not provide a built-in function for calculating the vector cross product for complex numbers, as the cross product is traditionally defined only for three-dimensional real vectors.&lt;/p&gt;</prompt-completion><code class="java">Please provide a code snippet implementing the vector cross product for complex numbers in the Eigen template library.</code><code class="java">&lt;p&gt;Eigen does not provide a built-in function for calculating the vector cross product for complex numbers, as the cross product is traditionally defined only for three-dimensional real vectors.&lt;/p&gt;</code></prompt-snippet><p>However, after <a href="https://eigen.tuxfamily.org/dox/group__Geometry__Module.html#ga0024b44eca99cb7135887c2aaf319d28" rel="noopener noreferrer nofollow" target="_blank">looking at the Eigen documentation</a>, it was found that the implementation for the vector cross product for complex numbers does exist, which shows that LLMs could hallucinate.</p><p>Relying on information from AI models can lead to inaccuracies, resulting in challenges such as misguided decisions or misunderstandings of critical insights. It is important to verify the accuracy of information manually due to the extensive use of AI in many sectors resulting in the increasing prevalence of AI-generated content.</p><p>Cross-verification and fact-checking help to eradicate hallucinations by comparing the results from AI with the authentic sources. Remember, this ensures that the information is trustworthy, keeping you safe from building upon erroneous foundations!</p><h5 id="input-and-output-limitations">Input and output limitations</h5><p>The model gets the idea from the input to generate the output. LLMs utilize the <strong>tokenization process</strong>, meaning that the input text is divided into individual units known as tokens in a way that allows the model to understand and process them effectively.</p><p>In the following example, each word and each comma has been counted as a single token. To count the characters, the spaces are also considered along with words and commas.</p><p style="text-align: center"><img alt="It depicts how to count tokens using OpenAI's method." height="353" src="https://lh7-us.googleusercontent.com/VRdpqCEX4Sbs__6ADDLC8vuF1NvCVmheCf4zFXsJIHlRXMLyyYWDG-lg6g4wZCfRWDOLrqt-vH0Z6-fJnZp6UiP4KlXCvyQPOg9C9oklN479yUkhhcWe4-lJI_ouLv1-Yt2C2FNlSrSbsXI392a8BLc" width="602"/></p><p>Below are the rough estimations of tokens provided by OpenAI:</p><ul><li><p>1 token is approximately equivalent to 4 characters in English</p></li><li><p>1 token is approximately equivalent to Â¾ words</p></li><li><p>100 tokens is approximately equivalent to 75 words</p></li></ul><p>The limitation in the number of input tokens reduces the excessive information to be transmitted to LLM. As a result, it allows the model to generate output within a confined range, enhancing accuracy based on the provided input. ChatGPT-3.5 has a context window of 4,096 tokens, whereas ChatGPT-4 has a context window of 8,192 tokens. ChatGPT-4o offers a context window of 128k, which allows larger input context.</p><div class="alert alert-warning"><p>Note: The pricing and token policies of models are subject to change due to the ongoing development of AI technologies. You are advised to check the latest information and pricing details on the <a href="https://openai.com/chatgpt/pricing/" rel="noopener noreferrer nofollow" target="_blank">official OpenAI pricing page</a>.</p></div><p>LLMs cannot work efficiently if the input tokens exceed their limit because this will hinder the model from remembering past information, affecting the response's coherence. If you want to know more about tokens and how to count them, you can read more in <a href="https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them" rel="noopener noreferrer nofollow" target="_blank">OpenAI's official documentation</a>.</p><p>The LLMs depend on the context window to generate the output coherently. The LLMs have '<em>short-term memory</em>', meaning they keep the context that lies in the current context window. Users have to remind it of important facts from previous prompts.</p><p>Different language models have their specific ways to count the tokens. So, for the rough estimation to count the token and to make sure that the input token lies within the limit, you could use a tool by OpenAI called "<a href="https://platform.openai.com/tokenizer" rel="noopener noreferrer nofollow" target="_blank"><u>Tokenizer</u></a>" to calculate the number of tokens sent as a prompt to the LLM for effective generation of response.<strong> </strong></p><h5 id="conclusion">Conclusion </h5><p>To use LLMs effectively, users have to be aware of their advantages and disadvantages for decision-making. It is advised to consider the limit of input tokens that you provide to the model because exceeding the token limits will lead to truncated outputs.</p><p>Although LLMs mostly generate logical responses, the facts still must be cross-verified due to the prevalence of challenges like hallucinations and knowledge cutoffs. Keeping these few simple rules in mind might save you lots of time and effort when working with generative AIs!<br/></p>
</div>